{"componentChunkName":"component---src-templates-blog-post-js","path":"/ML/knn/","result":{"data":{"site":{"siteMetadata":{"title":"DABI_devlog","author":"[DABIN SEO]","siteUrl":"https://Dabinnny.github.io","comment":{"disqusShortName":"","utterances":"Dabinnny/Dabinnny.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"5dbc8f4e-35bd-5fb2-b04c-5572f553c8ba","excerpt":"🍄 유유상종 KNN 분류하기 1. KNN(K-Nearest Nerghbor) 이란? 거리기반 분류분석 지도학습 모델 모델훈련이 필요없음 기존의 데이터와 새로운 데이터를 비교함으로써 새로운 데이터를 유사하게 판단된 기존 데이터로 분류 새로운 데이터가 입력되었을때, 새로운 데이터와 가장 인접한 기존의 데이터 k개를 선정 k개의 데이터 중 가장 빈도 수가 높게 나온 데이터의 레이블로 분류 K는 인접한 데이터의 갯수를 의미한다. 위의 사진처럼 K=1, K=…","html":"<h1 id=\"-유유상종-knn-분류하기\" style=\"position:relative;\"><a href=\"#-%EC%9C%A0%EC%9C%A0%EC%83%81%EC%A2%85-knn-%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0\" aria-label=\" 유유상종 knn 분류하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🍄 유유상종 KNN 분류하기</h1>\n<h2 id=\"1-knnk-nearest-nerghbor-이란\" style=\"position:relative;\"><a href=\"#1-knnk-nearest-nerghbor-%EC%9D%B4%EB%9E%80\" aria-label=\"1 knnk nearest nerghbor 이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. KNN(K-Nearest Nerghbor) 이란?</h2>\n<p><strong>거리기반 분류분석 지도학습 모델</strong> 모델훈련이 필요없음</p>\n<ul>\n<li>기존의 데이터와 새로운 데이터를 비교함으로써 새로운 데이터를 유사하게 판단된 기존 데이터로 분류</li>\n<li>새로운 데이터가 입력되었을때, 새로운 데이터와 가장 인접한 기존의 데이터 k개를 선정</li>\n<li>k개의 데이터 중 가장 빈도 수가 높게 나온 데이터의 레이블로 분류</li>\n</ul>\n<br/>\n<div align=\"center\"><img src=\"https://user-images.githubusercontent.com/90162819/160279499-10f8962d-b605-433e-9818-557629019356.png\" width=\"500\"> </div>  \n<br/>\n<p>K는 인접한 데이터의 갯수를 의미한다.<br>\n위의 사진처럼 K=1, K=3일때 분류값은 달라짐을 알수가 있다.<br>\n세모가 새로운 데이터라고 할때, K=1 일때 세모는 가장 가까운 별그룹으로 분류가 될것이다.<br>\n하지만 K=3 일때는 해당 해당 범위 안에서 가까운 종류는 동그라미 2, 별1 이다.<br>\n이때는 원이 갯수가 많기 때문에 세모는 동그라미 그룹으로 분류가 될것이다.<br>\n이렇듯, KNN 알고리즘은 K에 따라 결과가 달라지기 때문에 k를 정해주는 것이 중요하다.</p>\n<h2 id=\"2-거리-측정-방법\" style=\"position:relative;\"><a href=\"#2-%EA%B1%B0%EB%A6%AC-%EC%B8%A1%EC%A0%95-%EB%B0%A9%EB%B2%95\" aria-label=\"2 거리 측정 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 거리 측정 방법</h2>\n<h3 id=\"1-유클리드-거리-euclidean-distance\" style=\"position:relative;\"><a href=\"#1-%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%93%9C-%EA%B1%B0%EB%A6%AC-euclidean-distance\" aria-label=\"1 유클리드 거리 euclidean distance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1) 유클리드 거리 (Euclidean Distance)</h3>\n<p>일반적으로 사용하는 방법이다. 2차원 평면에 서로 다른 두 점 A(x1, y1)와 B(x2, y2)가 있을 때 이 둘의 거리 d는 다음과 같다.</p>\n<div align=\"center\"><img src=\"https://user-images.githubusercontent.com/90162819/160279774-1baa1a5f-6cd1-47c9-88ec-c2fbd6977383.png\" width=\"500\"> </div> \n<h3 id=\"2-맨해튼-거리-manfattan-distance\" style=\"position:relative;\"><a href=\"#2-%EB%A7%A8%ED%95%B4%ED%8A%BC-%EA%B1%B0%EB%A6%AC-manfattan-distance\" aria-label=\"2 맨해튼 거리 manfattan distance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2) 맨해튼 거리 (Manfattan Distance)</h3>\n<p>맨해튼 거리 모습에서 유래되었다. 직선으로 이동할 수 없는 상황에서 사용하는 방법이다.<br>\n점과 점사이의 직선거리가 아니라 x축, y축을 따라 간 거리를 의미한다.<br>\n검은색 두 점의 거리를 측정하려고 할 때, 초록색 직선이 유클리드 거리이며<br>\n나머지 빨간색, 파란색, 노란색 선이 모두 맨해튼 거리로 나타낸 것이다.</p>\n<div align=\"center\"><img src=\"https://user-images.githubusercontent.com/90162819/160279859-bae2b367-ded8-4bbf-bf2e-d7e507e8ed28.png\" width=\"400\"> </div>  \n<br/>\n<h2 id=\"3-knn의-장단점\" style=\"position:relative;\"><a href=\"#3-knn%EC%9D%98-%EC%9E%A5%EB%8B%A8%EC%A0%90\" aria-label=\"3 knn의 장단점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. KNN의 장단점</h2>\n<h3 id=\"1-장점\" style=\"position:relative;\"><a href=\"#1-%EC%9E%A5%EC%A0%90\" aria-label=\"1 장점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1) 장점</h3>\n<ul>\n<li>기존 분류 체계 값을 모두 검사하여 정확도가 높다</li>\n<li>비교시 가까운 거리의 상위 k개의 데이터만 활용하기 때문에 오류데이터가 결과에 영향을 미치지 않는다  </li>\n<li>기존 데이터를 기반으로 하기 때문에 학습 및 가정이 필요없다(빠르다)</li>\n</ul>\n<h3 id=\"2-단점\" style=\"position:relative;\"><a href=\"#2-%EB%8B%A8%EC%A0%90\" aria-label=\"2 단점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2) 단점</h3>\n<ul>\n<li>기존의 모든 데이터를 비교하므로 데이터가 많을수록 처리시간이 증가한다</li>\n<li>데이터를 많이 처리할때는 메모리 사용량이 많아 고사양의 하드웨어가 필요하다</li>\n<li>고차원의 데이터에 대해선 차원의 저주를 피하기 위해 차원 축소를 먼저 진행해야 한다  </li>\n</ul>\n<br/>\n<br/>\n<h2 id=\"4-최적의-k값-선택하기\" style=\"position:relative;\"><a href=\"#4-%EC%B5%9C%EC%A0%81%EC%9D%98-k%EA%B0%92-%EC%84%A0%ED%83%9D%ED%95%98%EA%B8%B0\" aria-label=\"4 최적의 k값 선택하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 최적의 K값 선택하기</h2>\n<p>k를 잘 선택하는것은 KNN의 성능을 결정하는 아주 중요한 요소이다.<br>\n일반적으로 k가 너무 작으면 데이터의 노이즈 성분까지 고려하는 과대적합(overfitting)문제가 발생한다.<br>\n반대로 k를 너무 크게 하면 결정함수가 너무 과하게 평탄화되어(oversmoothing) 데이터의 지역 정보를 예측할 수가 없다.  </p>\n<p>최적의 k 값을 찾기 위해서는 일반적인 규칙은 없지만 데이터의 특성과 상태에 따라 정할 수 있다.<br>\n데이터의 노이즈가 거의 없고 잘 구조화된 데이터의 경우 k 값이 작을수록 잘 동작한다.<br>\n반면, 노이즈가 많은 데이터의 경우 k가 클수록 좋다.<br>\nk는 보통 1~20 으로 지정하며 동률이 나오는 경우를 막기 위해 보통은 홀수를 사용한다.  </p>\n<br/>\n<h2 id=\"5-knn-예제\" style=\"position:relative;\"><a href=\"#5-knn-%EC%98%88%EC%A0%9C\" aria-label=\"5 knn 예제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. KNN 예제</h2>\n<p>sklearn의 내장데이터인 iris 데이터로 실습해보자.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> load_iris\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>neighbors <span class=\"token keyword\">import</span> KNeighborsClassifier\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>preprocessing <span class=\"token keyword\">import</span> StandardScaler\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> GridSearchCV\n\n<span class=\"token comment\"># 데이터셋 로드</span>\niris <span class=\"token operator\">=</span> load_iris<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nX<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> iris<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">,</span> iris<span class=\"token punctuation\">.</span>target<span class=\"token punctuation\">,</span> iris<span class=\"token punctuation\">.</span>target_names\n\n<span class=\"token comment\"># 학습/테스트 데이터셋 분할</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> stratify<span class=\"token operator\">=</span>y<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 데이터 전처리(표준화, Standardization)</span>\nstd <span class=\"token operator\">=</span> StandardScaler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nX_train_std <span class=\"token operator\">=</span> std<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span>\nX_test_std <span class=\"token operator\">=</span> std<span class=\"token punctuation\">.</span>transform<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># KNN + 그리드서치로 모델 학습</span>\nknn <span class=\"token operator\">=</span> KNeighborsClassifier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nparam_grid <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">'n_neighbors'</span><span class=\"token punctuation\">:</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>\ngs <span class=\"token operator\">=</span> GridSearchCV<span class=\"token punctuation\">(</span>estimator<span class=\"token operator\">=</span>knn<span class=\"token punctuation\">,</span> param_grid<span class=\"token operator\">=</span>param_grid<span class=\"token punctuation\">,</span> scoring<span class=\"token operator\">=</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">,</span> cv<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> n_jobs<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\ngs<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train_std<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 그리드서치 학습 결과 출력</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'최적 k값: {0}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>gs<span class=\"token punctuation\">.</span>best_params_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'최적 k값 일 때 정확도: {0:.2f}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>gs<span class=\"token punctuation\">.</span>best_score_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 최적화 모델 추출</span>\nmodel <span class=\"token operator\">=</span> gs<span class=\"token punctuation\">.</span>best_estimator_\n\n<span class=\"token comment\"># 테스트세트 정확도 출력</span>\nscore <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>score<span class=\"token punctuation\">(</span>X_test_std<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'테스트세트에서의 정확도: {0:.2f}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>score<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 테스트세트 예측 결과 샘플 출력</span>\npredicted_y <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test_std<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'실제 값: {0}, 예측 값: {1}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>labels<span class=\"token punctuation\">[</span>y_test<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">[</span>predicted_y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div align=\"center\"><img src=\"https://user-images.githubusercontent.com/90162819/160280630-14902ae3-bb93-43ee-b4b5-05aecb1f01ad.png\" width=\"450\"> </div> \n <br/>\n <br/>  \n <br/>\n <br/> \n<blockquote>\n<p>Reference</p>\n<ul>\n<li><a href=\"https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&#x26;blogId=baek2sm&#x26;logNo=221763552440&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false\">1</a></li>\n<li><a href=\"https://velog.io/@jhlee508/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-KNNK-Nearest-Neighbor-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98\">2</a></li>\n<li><a href=\"https://firework-ham.tistory.com/27\">3</a></li>\n</ul>\n</blockquote>","frontmatter":{"title":"KNN(K-Nearest Nerghbor) 최근접이웃 알고리즘","date":"October 02, 2021"}}},"pageContext":{"slug":"/ML/knn/","previous":{"fields":{"slug":"/ML/다중공선성/"},"frontmatter":{"title":"Multicollinearity(다중공선성) 문제와 해결하기"}},"next":{"fields":{"slug":"/Database/base/"},"frontmatter":{"title":"기본 쿼리 조회"}}}},"staticQueryHashes":["3128451518","635777304"]}