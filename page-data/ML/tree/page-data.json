{"componentChunkName":"component---src-templates-blog-post-js","path":"/ML/tree/","result":{"data":{"site":{"siteMetadata":{"title":"DABI_devlog","author":"[DABIN SEO]","siteUrl":"https://Dabinnny.github.io","comment":{"disqusShortName":"","utterances":"Dabinnny/Dabinnny.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"9617eacc-6884-5391-8dc9-ef29d52ddc8c","excerpt":"🌳 Tree 기반 앙상블 모델의 종류와 paraneter들에 대해서 알아보자 weak learnet은 bias-variance tradeoff를 야기한다. 대부분의 경우 편향이 높거나(과소적합) 분산이 높은(과대적합), 성능이 좋지 않은 모델이다. 더 견고한 모델을 만들기 위해 weak learner들을 결합함으로써 편향과 분산을 최적화시키고 성능을 올리는 것이 앙상블 방식의 최종 목표라고  할 수 있다.  이러한 앙상블 기법중 하나인 Boosting…","html":"<h1 id=\"-tree-기반-앙상블-모델의-종류와-paraneter들에-대해서-알아보자\" style=\"position:relative;\"><a href=\"#-tree-%EA%B8%B0%EB%B0%98-%EC%95%99%EC%83%81%EB%B8%94-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-paraneter%EB%93%A4%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90\" aria-label=\" tree 기반 앙상블 모델의 종류와 paraneter들에 대해서 알아보자 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🌳 Tree 기반 앙상블 모델의 종류와 paraneter들에 대해서 알아보자</h1>\n<br/> \n<br/> \n<div align=\"center\"><img src=\"https://i.imgur.com/pupI2V1.png\" width=\"500\"> </div> \n<br/> \n<br/>  \n<p>weak learnet은 bias-variance tradeoff를 야기한다.<br>\n대부분의 경우 편향이 높거나(과소적합) 분산이 높은(과대적합), 성능이 좋지 않은 모델이다.<br>\n더 견고한 모델을 만들기 위해 weak learner들을 결합함으로써 편향과 분산을 최적화시키고 성능을 올리는 것이 앙상블 방식의 최종 목표라고<br>\n할 수 있다.<br>\n이러한 앙상블 기법중 하나인 Boosting모델은 여러 개의 약한 모델을 순차적으로 학습-예측 해가면서, 잘못 예측한 데이터에 가중치를 부여해 오류를 줄여나가는 방법으로 순서가 있는 직렬 학습법이다.<br>\nBoosting 알고리즘을 사용하는 대표적인 모델로 AdaBoost와 GBM이 있으며, Boosting계열 알고리즘에서 가장 각광받고 있는 XGBoost와 LightGBM에 대해서 알아보자.  </p>\n<br/> \n<br/> \n<h2 id=\"1-xgboost\" style=\"position:relative;\"><a href=\"#1-xgboost\" aria-label=\"1 xgboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. XGBoost</h2>\n<div align=\"center\"><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/11194110/leaf.png\" width=\"650\"> </div> \n<br/> \n<ul>\n<li>여러개의 Decision Tree를 조합한 알고리즘으로 Gradient Boost를 병렬 학습이 가능하도록 구현된다. </li>\n</ul>\n<br/> \n<p><strong>(1) 장점</strong>  </p>\n<ul>\n<li>GBM 대비 빠른 속도  </li>\n<li>과적합 문제를 규제(Regularization)  </li>\n<li>조기종료(Early Stopping) 기능 </li>\n</ul>\n<br/> \n<p><strong>(2) 단점</strong></p>\n<ul>\n<li>하이터 파라미터가 다소 복잡(수가 많음)해서 많이 변형할 경우 학습시간이 길어지고 모델이 무거워짐  </li>\n</ul>\n<br/> \n<p><strong>(3) 하이퍼 파라미터 종류</strong>  </p>\n<p>가. 일반 파라미터  </p>\n<ul>\n<li><code class=\"language-text\">booster</code> [default = gbtree] : 어떤 부스터 구조를 쓸지 결정(의사결정기반모형(gbtree), 선형모형(gblinear), dart)</li>\n<li><code class=\"language-text\">n_jobs</code> : 실행하는 데 사용되는 병렬 스레드 수</li>\n<li><code class=\"language-text\">verbosity</code> [default = 1] : 유효한 값은 0 (무음), 1 (경고), 2 (정보), 3 (디버그) </li>\n</ul>\n<br/> \n<p>나. 부스터 파라미터  </p>\n<ul>\n<li><code class=\"language-text\">learning_rate</code> [ default : 0.3 ] : 높을수록 과적합 쉬움</li>\n<li><code class=\"language-text\">n_estimators</code> [ default : 100 ] : 생성할 weak learner의 수, 개수가 많을수록 수행시간 오래걸림, learning rate가 낮을 때는 n_estimators를 높여야 과적합 방지</li>\n<li><code class=\"language-text\">max_depth</code> [ default : 6 ]</li>\n<li><code class=\"language-text\">min_child_weight</code> [ default : 1 ] : 관측치에 대한 가중치 합의 최소</li>\n<li><code class=\"language-text\">gamma</code> [ default : 0 ] : 리프노드의 추가분할을 결정할 최소손실 감소값</li>\n<li><code class=\"language-text\">subsample</code> [ default : 1 ] : weak learner가 학습에 사용하는 데이터 샘플링 비율</li>\n<li><code class=\"language-text\">colsample_bytree</code> [ default : 1 ] : 각 tree 별 사용된 feature의 비율</li>\n<li><code class=\"language-text\">lambda</code> [default : 1] : 가중치에 대한 L2 적용 값으로 피쳐개수가 많으면 적용하고 값이 클수록 과적합 감소효과</li>\n<li><code class=\"language-text\">alpha</code> [default : 0] : 가중치에 대한 L1 적용 값</li>\n</ul>\n<br/> \n<p>다. 학습 과정 파라미터  </p>\n<ul>\n<li><code class=\"language-text\">objective</code> [ default : reg = squarederror ]</li>\n<li><code class=\"language-text\">binary</code> : logistic (binary-logistic classification)</li>\n<li><code class=\"language-text\">multi</code> : softmax (num_class 지정)</li>\n<li><code class=\"language-text\">multi</code> : softprob (각 클래스 범주에 속하는 예측확률을 반환)</li>\n<li><code class=\"language-text\">count</code> : poisson (count data poison regression) 등 다양</li>\n<li><code class=\"language-text\">eval_metric</code> : rmse, mae, logloss, error, merror, auc</li>\n<li><code class=\"language-text\">seed</code> [ default : 0 ]</li>\n</ul>\n<br/> \n<p>라. 주요 파라미터 : learning rate, max<em>depth, n</em>estinators, subsample, min<em>split</em>loss, early<em>stopping</em>rounds, alpha, lambda  </p>\n<br/> \n<h2 id=\"2-lightgbm\" style=\"position:relative;\"><a href=\"#2-lightgbm\" aria-label=\"2 lightgbm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. LightGBM</h2>\n<div align=\"center\"><img src=\"https://entheoscientist.files.wordpress.com/2020/04/e37e7-1azssoxb8lc5n6mnhqx5jcg.png\" width=\"700\"> </div> \n<br/> \n<ul>\n<li>여러개의 Decision Tree를 조합한 알고리즘으로 Gradient Boost를 병렬 학습이 가능하도록 구현된다. </li>\n</ul>\n<br/> \n<p><strong>(1) 장점</strong>  </p>\n<ul>\n<li>빠른 학습 및 수행시간</li>\n<li>더 작은 메모리 사용량</li>\n<li>범주형 feature의 자동 변환 및 최적 분할</li>\n</ul>\n<br/> \n<p><strong>(2) 단점</strong></p>\n<ul>\n<li>작은 dataset을 사용할 경우 과적합 가능성 (일반적으로 10,000개 이하의 데이터)</li>\n</ul>\n<br/> \n<p><strong>(3) 하이퍼 파라미터 종류</strong>  </p>\n<ul>\n<li>XGBoost와 많은 부분이 유사하다. 주의할 점은 리프 중심 트리 분할 방식으로 트리가 깊어지므로 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다.  </li>\n</ul>\n<br/> \n<p>가. 주요 파라미터  </p>\n<ul>\n<li><code class=\"language-text\">n_estimators</code> [ default : 100 ] : 생성할 weak learner의 수</li>\n<li><code class=\"language-text\">learning_rate</code> [ default : 0.1 ] : 일반적으로 n<em>estimators를 크게 하고 learning</em>rate를 작게 해서 예측 성능을 향상</li>\n<li><code class=\"language-text\">max_depth</code> [ default : -1 ] : Leaf wise 기반이므로 깊이가 상대적으로 더 깊음</li>\n<li><code class=\"language-text\">min_data_in_leat</code> [ default : 20 ] : 최종 결정 클래스인 리프 노드가 되기 위해서 최소한으로 필요한 레코드 수, 과적합을 제어</li>\n<li><code class=\"language-text\">num_leaves</code> [ default : 31 ] : 최대 리프 개수</li>\n<li><code class=\"language-text\">boosting</code> [default = gbdt] : 어떤 부스터 구조를 쓸지 결정(gbdt, rf)</li>\n<li><code class=\"language-text\">bagging_fraction</code> [ default : 1 ]: weak learner가 학습에 사용하는 데이터 샘플링 비율</li>\n<li><code class=\"language-text\">feature_fraction</code> [ default : 1 ]: 개별 트리를 학습할 때마다 무작위로 선택하는 feature의 비율 </li>\n<li><code class=\"language-text\">lambda_l2</code> [ default : 0 ]: L2 제어값</li>\n<li><code class=\"language-text\">lambda l1</code> [ default : 0 ]: L1 제어값  </li>\n</ul>\n<br/> \n<p>나. Learning Task 파라미터</p>\n<ul>\n<li><code class=\"language-text\">objective</code> : 최소값을 가져야 할 손실함수 정의, 회귀, 다중 클래스 분류, 이진 분류인지에 따라서 objective인 손실 함수가 지정  </li>\n</ul>\n<br/> \n<br/> \n<br/> \n<br/> \n<blockquote>\n<p>Reference  </p>\n</blockquote>\n<ul>\n<li><a href=\"https://wooono.tistory.com/97\">https://wooono.tistory.com/97</a></li>\n<li><a href=\"https://velog.io/@sset2323/04-07.-LightGBM\">https://velog.io/@sset2323/04-07.-LightGBM</a></li>\n</ul>","frontmatter":{"title":"Tree Based Model(1) - XGBoost, LigntGBM","date":"April 06, 2022"}}},"pageContext":{"slug":"/ML/tree/","previous":{"fields":{"slug":"/ETC/pandas/"},"frontmatter":{"title":"Mac M1 pandas cpython-38-darwin.so 에러 "}},"next":{"fields":{"slug":"/Database/base/"},"frontmatter":{"title":"[programmers] 있었는데요 없었습니다(JOIN)"}}}},"staticQueryHashes":["3128451518","635777304"]}